{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Different Types of Learning Rules in ANN are as following below:**\n",
        "\n",
        "1. Hebbian Leaning\n",
        "2. Error Correction Rule(Adaline Learning)\n",
        "3. Competitive Learning\n",
        "4. Memory Based Learning"
      ],
      "metadata": {
        "id": "1WhQFJDl5rEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hebbian Learning**\n",
        "1. Set all weight to zero and bias also zero.\n",
        "2. Repeat step 3 and 4 until stopping Condition is false\n",
        "3. For Each training pair (s,t)\n",
        "* Calculate the net input at output:\n",
        "    * y(output)= bias + sum(wi*xi) where i=1 to n\n",
        "4. Update the weight and bias if input not equal to target\n",
        "    * wi(new) = wi(old) + xi*t\n",
        "    * b(new) = b(new) + t\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B6cHQHt-litB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND & OR Gate using Hebbian Learning"
      ],
      "metadata": {
        "id": "juRSJQYM_Pea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x1=np.array([-1,-1,1,1])\n",
        "x2=np.array([-1,1,-1,1])\n",
        "t_AND=np.array([-1,-1,-1,1])\n",
        "t_OR=np.array([-1,1,1,1])\n",
        "\n",
        "def Hebb_learning(x1, x2, t, w1, w2, b):\n",
        "    for i in range(len(x1)):\n",
        "        y = w1 * x1[i] + w2 * x2[i] + b\n",
        "        if y != t[i]:\n",
        "            w1 += t[i] * x1[i]\n",
        "            w2 += t[i] * x2[i]\n",
        "            b += t[i]\n",
        "    return w1, w2, b\n",
        "\n",
        "def AND(x1, x2, w1, w2, b):\n",
        "    weighted_sum = w1 * x1 + w2 * x2 + b\n",
        "    if weighted_sum >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def OR(x1, x2, w1, w2, b):\n",
        "    weighted_sum = w1 * x1 + w2 * x2 + b\n",
        "    if weighted_sum >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "F4CZpeWAlfxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AND Gate Using Hebbian Learning\\n\")\n",
        "w1, w2, b = 0, 0, 0\n",
        "w1, w2, b = Hebb_learning(x1, x2, t_AND, w1, w2, b)\n",
        "\n",
        "# Test the AND function\n",
        "print(\"-1 AND -1:\", AND(-1, -1, w1, w2, b))\n",
        "print(\"-1 AND  1:\", AND(-1, 1, w1, w2, b))\n",
        "print(\" 1 AND -1:\", AND(1, -1, w1, w2, b))\n",
        "print(\" 1 AND  1:\", AND(1, 1, w1, w2, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ERSufHBjmB",
        "outputId": "52745a5e-44d2-4c1e-b202-2c499675c6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate Using Hebbian Learning\n",
            "\n",
            "-1 AND -1: -1\n",
            "-1 AND  1: -1\n",
            " 1 AND -1: -1\n",
            " 1 AND  1: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"OR Gate Using Hebbian Learning\\n\")\n",
        "w1, w2, b = 0, 0, 0\n",
        "w1, w2, b = Hebb_learning(x1, x2, t_OR, w1, w2, b)\n",
        "\n",
        "# Test the OR function\n",
        "print(\"-1 OR -1:\", OR(-1, -1, w1, w2, b))\n",
        "print(\"-1 OR  1:\", OR(-1, 1, w1, w2, b))\n",
        "print(\" 1 OR -1:\", OR(1, -1, w1, w2, b))\n",
        "print(\" 1 OR  1:\", OR(1, 1, w1, w2, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D620Arq7CStq",
        "outputId": "a24ddff5-ae5e-4380-b8f8-f4b4e6c2f523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate Using Hebbian Learning\n",
            "\n",
            "-1 OR -1: -1\n",
            "-1 OR  1: 1\n",
            " 1 OR -1: 1\n",
            " 1 OR  1: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error Correction Rule(Adaline Learning)**\n",
        "1. Initiliaze the weight(w),learning rate(eta),bias(b) to small random value\n",
        "2.  Repeat the step 3 and 4 until stopping condition is false Es\n",
        "3.   For each training pair(s,t)\n",
        "4.  Activate the input i=1 to n such that x(i)=S(i)\n",
        "5. calculate the net input at output\n",
        "   *   y_in=b_+sum( w_i*x_i) where i=1 to n\n",
        "6.   Calculate the error\n",
        "   *   E_i=(t-y_in)\n",
        "7.  Update the weight and bias are\n",
        "   *   w_i(n)=w_i(old)+eta* (t-y_in*x_i)\n",
        "   *   b_i(n)=b_i(old)+eta* (t-y_in)\n",
        "8.   If the E_i=E_s then stopping condition set a threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "6lh9J8xACwA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OR Gate using Adaline**"
      ],
      "metadata": {
        "id": "4aEzy2OEHOGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bipolar OR gate input patterns and expected outputs\n",
        "x1 = np.array([1, 1, -1, -1])\n",
        "x2 = np.array([1, -1, 1, -1])\n",
        "t = np.array([1, 1, 1, -1])\n",
        "\n",
        "# Initialize weights, bias, learning rate, total_error, and iteration\n",
        "weight_1 = 0.1\n",
        "weight_2 = 0.1\n",
        "bias = 0.1\n",
        "learning_rate = 0.1\n",
        "iteration = 0\n",
        "\n",
        "# OR Gate using Adaline Learning rate\n",
        "while True:\n",
        "    total_error = 0\n",
        "    print(f\"Epoch: {iteration+1}\\n\")\n",
        "    print(f\"\\tInput\\tTarget\\tYin\\tError\\tW1\\tW2\\tBias\\tFinal Error\\tTotal Error\")\n",
        "\n",
        "    for i in range(4):\n",
        "        y_in = bias + weight_1 * x1[i] + weight_2 * x2[i]\n",
        "        error = t[i] - y_in\n",
        "        final_error = error ** 2\n",
        "        total_error += final_error\n",
        "        weight_1 += learning_rate * error * x1[i]\n",
        "        weight_2 += learning_rate * error * x2[i]\n",
        "        bias += learning_rate * error\n",
        "        print(f\"\\t{x1[i], x2[i]}\\t{t[i]}\\t{y_in:.4f}\\t{error:.4f}\\t{weight_1:.4f}\\t{weight_2:.4f}\\t{bias:.4f}\\t{final_error:.4f}\\t\\t{total_error:.4f}\")\n",
        "    iteration += 1\n",
        "    print(\"\\n\")\n",
        "    if total_error <= 2:\n",
        "        break\n",
        "\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weight 1: {weight_1:.4f}\\nWeight 2: {weight_2:.4f}\\nBias: {bias:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B_DB3HBEDQ2",
        "outputId": "69a6d7a6-7804-4221-a227-26fe311a069d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "\tInput\tTarget\tYin\tError\tW1\tW2\tBias\tFinal Error\tTotal Error\n",
            "\t(1, 1)\t1\t0.3000\t0.7000\t0.1700\t0.1700\t0.1700\t0.4900\t\t0.4900\n",
            "\t(1, -1)\t1\t0.1700\t0.8300\t0.2530\t0.0870\t0.2530\t0.6889\t\t1.1789\n",
            "\t(-1, 1)\t1\t0.0870\t0.9130\t0.1617\t0.1783\t0.3443\t0.8336\t\t2.0125\n",
            "\t(-1, -1)\t-1\t0.0043\t-1.0043\t0.2621\t0.2787\t0.2439\t1.0086\t\t3.0211\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "\tInput\tTarget\tYin\tError\tW1\tW2\tBias\tFinal Error\tTotal Error\n",
            "\t(1, 1)\t1\t0.7847\t0.2153\t0.2837\t0.3003\t0.2654\t0.0463\t\t0.0463\n",
            "\t(1, -1)\t1\t0.2488\t0.7512\t0.3588\t0.2251\t0.3405\t0.5643\t\t0.6106\n",
            "\t(-1, 1)\t1\t0.2069\t0.7931\t0.2795\t0.3044\t0.4198\t0.6290\t\t1.2397\n",
            "\t(-1, -1)\t-1\t-0.1641\t-0.8359\t0.3631\t0.3880\t0.3362\t0.6988\t\t1.9384\n",
            "\n",
            "\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weight 1: 0.3631\n",
            "Weight 2: 0.3880\n",
            "Bias: 0.3362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AND Gate using Adaline**"
      ],
      "metadata": {
        "id": "R_2-U716HQwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([1, 1, -1, -1])\n",
        "x2 = np.array([1, -1, 1, -1])\n",
        "t = np.array([-1,-1,-1,1])\n",
        "\n",
        "weight_1 = 0.1\n",
        "weight_2 = 0.1\n",
        "bias = 0.1\n",
        "learning_rate = 0.1\n",
        "iteration = 0\n",
        "\n",
        "while True:\n",
        "    total_error = 0\n",
        "    print(f\"Epoch: {iteration+1}\\n\")\n",
        "    print(f\"\\tInput\\tTarget\\tYin\\tError\\tW1\\tW2\\tBias\\tFinal Error\\tTotal Error\")\n",
        "\n",
        "    for i in range(4):\n",
        "        y_in = bias + weight_1 * x1[i] + weight_2 * x2[i]\n",
        "        error = t[i] - y_in\n",
        "        final_error = error ** 2\n",
        "        total_error += final_error\n",
        "        weight_1 += learning_rate * error * x1[i]\n",
        "        weight_2 += learning_rate * error * x2[i]\n",
        "        bias += learning_rate * error\n",
        "        print(f\"\\t{x1[i], x2[i]}\\t{t[i]}\\t{y_in:.4f}\\t{error:.4f}\\t{weight_1:.4f}\\t{weight_2:.4f}\\t{bias:.4f}\\t{final_error:.4f}\\t\\t{total_error:.4f}\")\n",
        "    iteration += 1\n",
        "    print(\"\\n\")\n",
        "    if total_error <= 2:\n",
        "        break\n",
        "\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weight 1: {weight_1:.4f}\\nWeight 2: {weight_2:.4f}\\nBias: {bias:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OTVWNKbHJje",
        "outputId": "4285897c-bd5a-4597-b4b4-e195791bca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\n",
            "\tInput\tTarget\tYin\tError\tW1\tW2\tBias\tFinal Error\tTotal Error\n",
            "\t(1, 1)\t-1\t0.3000\t-1.3000\t-0.0300\t-0.0300\t-0.0300\t1.6900\t\t1.6900\n",
            "\t(1, -1)\t-1\t-0.0300\t-0.9700\t-0.1270\t0.0670\t-0.1270\t0.9409\t\t2.6309\n",
            "\t(-1, 1)\t-1\t0.0670\t-1.0670\t-0.0203\t-0.0397\t-0.2337\t1.1385\t\t3.7694\n",
            "\t(-1, -1)\t1\t-0.1737\t1.1737\t-0.1377\t-0.1571\t-0.1163\t1.3776\t\t5.1470\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "\n",
            "\tInput\tTarget\tYin\tError\tW1\tW2\tBias\tFinal Error\tTotal Error\n",
            "\t(1, 1)\t-1\t-0.4111\t-0.5889\t-0.1966\t-0.2160\t-0.1752\t0.3468\t\t0.3468\n",
            "\t(1, -1)\t-1\t-0.1558\t-0.8442\t-0.2810\t-0.1315\t-0.2596\t0.7126\t\t1.0595\n",
            "\t(-1, 1)\t-1\t-0.1102\t-0.8898\t-0.1920\t-0.2205\t-0.3486\t0.7917\t\t1.8512\n",
            "\t(-1, -1)\t1\t0.0639\t0.9361\t-0.2856\t-0.3141\t-0.2550\t0.8763\t\t2.7275\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "\n",
            "\tInput\tTarget\tYin\tError\tW1\tW2\tBias\tFinal Error\tTotal Error\n",
            "\t(1, 1)\t-1\t-0.8548\t-0.1452\t-0.3001\t-0.3287\t-0.2695\t0.0211\t\t0.0211\n",
            "\t(1, -1)\t-1\t-0.2410\t-0.7590\t-0.3760\t-0.2528\t-0.3454\t0.5761\t\t0.5972\n",
            "\t(-1, 1)\t-1\t-0.2222\t-0.7778\t-0.2982\t-0.3305\t-0.4232\t0.6050\t\t1.2022\n",
            "\t(-1, -1)\t1\t0.2056\t0.7944\t-0.3777\t-0.4100\t-0.3438\t0.6311\t\t1.8333\n",
            "\n",
            "\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weight 1: -0.3777\n",
            "Weight 2: -0.4100\n",
            "Bias: -0.3438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memory Based Learning**"
      ],
      "metadata": {
        "id": "H__DFSOlJkHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The basic idea behind memory-based learning is that concepts can be classified by their similarity with previously seen concepts.\n",
        "* In a memory-based system, learning amounts to storing the training data items.\n",
        "* The strength of such a system lies in its capability to compute the similarity between a new data item and the training data items."
      ],
      "metadata": {
        "id": "ZpzQGAZDKC3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(vector1, vector2):\n",
        "    return np.sum((vector1 - vector2) ** 2)\n",
        "\n",
        "def memory_based(training_data, input_data):\n",
        "    distance_output_map = {}\n",
        "    for sample_input, sample_output in training_data:\n",
        "        distance = euclidean_distance(input_data, sample_input)\n",
        "        if distance not in distance_output_map or distance_output_map[distance] > sample_output:\n",
        "            distance_output_map[distance] = sample_output\n",
        "    min_distance = min(distance_output_map.keys())\n",
        "    return distance_output_map[min_distance]"
      ],
      "metadata": {
        "id": "190kc45VKPTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [(np.array([0, 0]), 0), (np.array([0, 1]), 0), (np.array([1, 0]), 0), (np.array([1, 1]), 1)]\n",
        "test_data= np.array([1, 0])\n",
        "print(f\"Test data: {test_data}\")\n",
        "result = memory_based(training_data, test_data)\n",
        "print(f\"\\n Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BYpnYZgZSu-",
        "outputId": "92253a1d-5b28-4799-ef15-1d6a6ae97655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data: [1 0]\n",
            "\n",
            " Result: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "    (np.array([0, 0, 0]), 0),\n",
        "    (np.array([0, 0, 1]), 0),\n",
        "    (np.array([0, 1, 0]), 0),\n",
        "    (np.array([0, 1, 1]), 1),\n",
        "    (np.array([1, 0, 0]), 0),\n",
        "    (np.array([1, 1, 0]), 1),\n",
        "    (np.array([1, 1, 1]), 0)\n",
        "]\n",
        "\n",
        "test_data = np.array([1, 0, 1])\n",
        "print(f\"Test data: {test_data}\")\n",
        "result = memory_based(training_data, test_data)\n",
        "print(f\"\\n Result: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKWG9WSXZWD3",
        "outputId": "55308ceb-1b8f-4643-9727-9a2b482a766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data: [1 0 1]\n",
            "\n",
            " Result: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Competitive Learning**"
      ],
      "metadata": {
        "id": "DgIVO-kCHnrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For a Neuron k to be the winning neuron, the Net input for a set of input patterns must be the largest among all the neurons in the network.\n",
        "* and a Neuron learn by using\n",
        "  * Delta(wk)= eta*(xj-wkj) if k wins elese 0\n",
        "  ,where eta is the learning rate\n",
        "\n"
      ],
      "metadata": {
        "id": "mABTh1dfIp34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector1 = np.array([1,1,0,0])\n",
        "input_vector2=np.array([0,0,0,1])\n",
        "input_vector3=np.array([0,0,1,1])\n",
        "input_vector4=np.array([1,1,0,0])\n",
        "weights_y1 = np.array([0.2, 0.6, 0.5, 0.9])\n",
        "weights_y2 = np.array([0.8, 0.4, 0.7, 0.3])\n",
        "learning_rate = 0.6"
      ],
      "metadata": {
        "id": "Zt7GiZGzHt9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=[input_vector1,input_vector2,input_vector3,input_vector4]\n",
        "input[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7aTpANFIclu",
        "outputId": "b23a7c22-f8b5-415f-e4b3-7d892c11a5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance_squared(vector1, vector2):\n",
        "    return np.sum((vector1 - vector2) ** 2)"
      ],
      "metadata": {
        "id": "Jv5lgBi8IKeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SOM(input_vector,weights_y1,weights_y2):\n",
        "  distance_y1 =  euclidean_distance_squared(input_vector, weights_y1)\n",
        "  distance_y2 =  euclidean_distance_squared(input_vector, weights_y2)\n",
        "  winning_neuron = 1 if distance_y1 < distance_y2 else 2\n",
        "  print(\"Winning Neuron is : \",winning_neuron)\n",
        "  if winning_neuron == 1:\n",
        "    weights_y1 = weights_y1 + learning_rate * (input_vector - weights_y1)\n",
        "  else:\n",
        "    weights_y2 = weights_y2 + learning_rate * (input_vector - weights_y2)\n",
        "  return weights_y1,weights_y2\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "  print(\"Input : \",input[i])\n",
        "  weights_y1,weights_y2=SOM(input[i],weights_y1,weights_y2)\n",
        "  print(\"Updated weights for y1:\", weights_y1)\n",
        "  print(\"Updated weights for y2:\", weights_y2)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tnW2wgMINTV",
        "outputId": "d2f2fa3e-e81e-4175-bcb4-9ed6663afce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input :  [1 1 0 0]\n",
            "Winning Neuron is :  2\n",
            "Updated weights for y1: [0.2 0.6 0.5 0.9]\n",
            "Updated weights for y2: [0.92 0.76 0.28 0.12]\n",
            "\n",
            "Input :  [0 0 0 1]\n",
            "Winning Neuron is :  1\n",
            "Updated weights for y1: [0.08 0.24 0.2  0.96]\n",
            "Updated weights for y2: [0.92 0.76 0.28 0.12]\n",
            "\n",
            "Input :  [0 0 1 1]\n",
            "Winning Neuron is :  1\n",
            "Updated weights for y1: [0.032 0.096 0.68  0.984]\n",
            "Updated weights for y2: [0.92 0.76 0.28 0.12]\n",
            "\n",
            "Input :  [1 1 0 0]\n",
            "Winning Neuron is :  2\n",
            "Updated weights for y1: [0.032 0.096 0.68  0.984]\n",
            "Updated weights for y2: [0.968 0.904 0.112 0.048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "1. Hebbian Learning: The weight updates in Hebbian learning might not necessarily converge to a specific value. This is because Hebbian learning only strengthens existing connections based on co-occurrence of inputs and doesn't account for errors.\n",
        "\n",
        "2. Delta Learning:Delta learning (also known as Least Mean Squares) usually converges to a minimum error value, making it a more robust learning rule compared to Hebbian or Perceptron learning for many scenarios.\n",
        "3. Memory based Learning: Memory-based learning, also known as instance-based learning or lazy learning, stores instances of training data and uses them directly during the testing phase. Unlike Hebbian learning, which strengthens connections based on co-occurrence of inputs without accounting for errors, memory-based learning directly uses stored instances to make predictions.\n",
        "\n",
        "4. Competitive Learning or SOM:Self-Organizing Maps (SOMs) : Neurons compete to represent input patterns, with the winning neuron and its neighbors adjusting to become more similar to the input.Self-organizing and effective for unsupervised learning and dimensionality reduction."
      ],
      "metadata": {
        "id": "WjrUewvcasE2"
      }
    }
  ]
}